<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kick Your Ass</title>
    <description>是一个高尚的人，一个纯粹的人，一个有道德的人，一个坚决不脱离低级趣味的人。</description>
    <link>http://heisenbean.me/</link>
    <atom:link href="http://heisenbean.me/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 26 Nov 2016 17:22:44 +0800</pubDate>
    <lastBuildDate>Sat, 26 Nov 2016 17:22:44 +0800</lastBuildDate>
    <generator>Jekyll v3.2.1</generator>
    
      <item>
        <title>手把手教你爬取豆瓣相册</title>
        <description>&lt;p&gt;Python版本:&lt;a href=&quot;https://www.python.org/downloads/&quot;&gt;2.7&lt;/a&gt;&lt;br /&gt;
开发工具:&lt;a href=&quot;https://www.jetbrains.com/pycharm/download/&quot;&gt;Pycharm Community for Mac&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我从简单到复杂,从用Python自带库到用到三方库来慢慢讲解怎么写这个爬虫.&lt;br /&gt;
最好有其他语言基础,有正则基础.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;最终目标:&lt;br /&gt;
 ✅ 动态加载网页爬取图片 &lt;br /&gt;
 ✅ 翻页处理&lt;br /&gt;
 ✅ 异常处理&lt;/p&gt;

&lt;p&gt;###&lt;a name=&quot;markdown-通过本地HTML网页爬取图片&quot;&gt;&lt;/a&gt;通过本地HTML网页爬取图片&lt;/p&gt;

&lt;p&gt;####1. 分析静态网页源码&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pycharm新建项目.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;随便找一个豆瓣相册地址&lt;code&gt;https://www.douban.com/photos/album/102879847/&lt;/code&gt;&lt;br /&gt;
把网页源代码复制一下,在项目路径下vim一个新文件后缀名为&lt;code&gt;.txt&lt;/code&gt;,&lt;code&gt;html&lt;/code&gt;都可以的,然后把复制的网页源码粘贴进去,这里我的源文件为&lt;code&gt;source.html&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;分析源码可看到相册里图片的url都在&lt;code&gt;&amp;lt;div class=&quot;photolst clearfix&quot;&amp;gt;&lt;/code&gt;和&lt;code&gt;&amp;lt;div id=&quot;link-report&quot;&lt;/code&gt;之间,先不要管这两个东西是什么,只要能找到这个规律就行了.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;那我们要做的就是通过代码拿到所有的图片url,然后下载到电脑上.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;####2. 读取源码,获取图片url&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;通过下面几句代码就能读取到源代码了.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  # -*- coding: utf-8 -*-
  file = open('source.html','r')
  html = file.read()
  file.close()
  print html   &amp;gt; `	# -*- coding: utf-8 -*-`由于编码问题,要加上这句声明,不然代码中出现中文会报错.
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;下面将要用到正则表达式的一些知识来获取我们想要的内容.上面提到过所有图片的url都在&lt;code&gt;&amp;lt;div class=&quot;photolst clearfix&quot;&amp;gt;&lt;/code&gt;和&lt;code&gt;&amp;lt;div id=&quot;link-report&quot;&lt;/code&gt;之间,所以我们通过正则可以这样拿到图片url.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  # -*- coding: utf-8 -*-
  # 引入正则库
  import re 
  file = open('source.html','r')
  html = file.read()
  file.close()
  # 截取想要的部分
  images = re.findall('&quot;photolst clearfix&quot;&amp;gt;(.*?)&amp;lt;div id=&quot;link-report',html,re.S)[0]
  print images
&lt;/code&gt;&lt;/pre&gt;

    &lt;blockquote&gt;
      &lt;p&gt;&lt;code&gt;.*?&lt;/code&gt;就是正则里的任意匹配,比如&lt;code&gt;abcdefg&lt;/code&gt;你想要&lt;code&gt;a&lt;/code&gt;和&lt;code&gt;g&lt;/code&gt;之间的字符,那么就是&lt;code&gt;'a(.*?)g'&lt;/code&gt;&lt;/p&gt;
    &lt;/blockquote&gt;

    &lt;p&gt;print结果中的其中一条数据:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;     &amp;lt;div class=&quot;photo_wrap&quot;&amp;gt;
          &amp;lt;a href=&quot;https://www.douban.com/photos/photo/2230900788/&quot; class=&quot;photolst_photo&quot; title=&quot;&quot;&amp;gt;
          &amp;lt;img src=&quot;https://img3.doubanio.com/view/photo/thumb/public/p1972176302.jpg&quot; /&amp;gt;
          &amp;lt;/a&amp;gt;&amp;lt;br/&amp;gt;
          &amp;lt;div class=&quot;pl&quot;&amp;gt;&amp;lt;/div&amp;gt;
          &amp;lt;div style=&quot;color:#999&quot;&amp;gt;
          &amp;lt;/div&amp;gt;
      &amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

    &lt;p&gt;通过打印发现此时拿到并不是纯粹的url,而我们想要的只是&lt;code&gt;img src&lt;/code&gt;标签后面的url.&lt;br /&gt;
那么还可以继续利用正则匹配拿到,思路和上面一样.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  # -*- coding: utf-8 -*-
  # 引入正则库
  import re 
	
  file = open('source.html','r')
  html = file.read()
  file.close()
  # 截取想要的部分
  images = re.findall('&quot;photolst clearfix&quot;&amp;gt;(.*?)&amp;lt;div id=&quot;link-report',html,re.S)[0]
  # 截取url
  pic_url = re.findall('img src=&quot;(.*?)&quot; /',images)
  print pic_url
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;####3. 通过图片url下载图片&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;现在已经拿到所有图片的url,并且已经在一个数组里了,那剩下的就是遍历这个数组,把图片一个一个下载下来.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  # -*- coding: utf-8 -*-
  # 引入正则库
  import re 
  # 网络请求库
  import requests
	
  file = open('source.html','r')
  html = file.read()
  file.close()
	
  images = re.findall('&quot;photolst clearfix&quot;&amp;gt;(.*?)&amp;lt;div id=&quot;link-report',html,re.S)[0]
  pic_url = re.findall('img src=&quot;(.*?)&quot; /',images)
	
  i = 0
  for url in pic_url:
      print '正在下载第' + str(i) + '张图片' 
      pic = requests.get(url)
      fp = open(str(i) + '.jpg','wb')
      fp.write(pic.content)
      fp.close()
      i += 1
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;但是现在下载的图片都是存放到当前文件夹了,期望的效果是自动创建一个文件夹并把图片下载到里面,也很好实现.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  # -*- coding: utf-8 -*-
  import re
  import os
  import requests
  file = open('source.html','r')
  html = file.read()
  file.close()
	
  images = re.findall('&quot;photolst clearfix&quot;&amp;gt;(.*?)&amp;lt;div id=&quot;link-report',html,re.S)[0]
  pic_url = re.findall('img src=&quot;(.*?)&quot; /',images)
	
  i = 0
  # 如果不存在此文件夹才创建
  if not os.path.exists('pic'):
      os.makedirs('pic')
      for url in pic_url:
          print '正在下载第' + str(i) + '张图片'
          pic = requests.get(url)
          fp = open(os.getcwd() + '/pic/' + str(i) + '.jpg', 'wb')
          # fp = open(str(i) + '.jpg','wb')
          fp.write(pic.content)
          fp.close()
          i += 1  
  else:
      print('您已下载过了') &amp;gt; `os.getcwd`是获取当前路径,以及`makedirs`都是`os`库里的方法,要记得引用os库.
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;至此,从静态html源代码爬取图片地址并下载算是已经做完了,接下来就直接通过网址来爬取图片.
###通过网址动态爬取图片
####1. 解析相url地址,获取HTML源码&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;还以&lt;code&gt;https://www.douban.com/photos/album/102879847/&lt;/code&gt;这个相册地址为例.通过网址来把HTML源码解析出来很简单,首先引用&lt;code&gt;urllib2&lt;/code&gt;库,然后打开,读取.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  album_url = 'https://www.douban.com/photos/album/102879847/'
  html = first_html = urllib2.urlopen(album_url).read()
  print html 既然能获取到HTML源码,那剩下的就和上面的[通过本地HTML网页爬取图片](#markdown-通过本地HTML网页爬取图片)一样了.
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;####2. 翻页处理&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;当在豆瓣相册我翻页的时候留意了地址的变化.
    &lt;ul&gt;
      &lt;li&gt;第一页 &lt;code&gt;https://www.douban.com/photos/album/102879847/&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;第二页 &lt;code&gt;https://www.douban.com/photos/album/102879847/?start=18&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;第三页 &lt;code&gt;https://www.douban.com/photos/album/102879847/?start=36&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;第n页 &lt;code&gt;https://www.douban.com/photos/album/102879847/?start=18*(n-1)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;不难发现其中的规律,就是在原来的基础上加了&lt;code&gt;?start=x&lt;/code&gt;,这个&lt;code&gt;x&lt;/code&gt;为18的倍数,因为每一页至多有18张照片,那么第一页后面应该也是有的&lt;code&gt;?start=0&lt;/code&gt;,只不过第一页作为缺省值直接隐藏掉了.&lt;/p&gt;

    &lt;p&gt;那这就好办了,先把思路理好.我们把有照片的页码相册地址都放到数组里&lt;code&gt;[album_1,album_2,album_3]&lt;/code&gt;,然后遍历出每一个相册的地址,拿到相册地址,就能接着上面的继续做了.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;分析每一页的源码&lt;br /&gt;
  上面有讲到,照片的url都在源码中的&lt;code&gt;&amp;lt;div class=&quot;photolst clearfix&quot;&amp;gt;&lt;/code&gt;和&lt;code&gt;&amp;lt;div id=&quot;link-report&quot;&lt;/code&gt;之间.我们先假设从他们之间截取的字符串的值为&lt;code&gt;images&lt;/code&gt;,那么如果一个相册只有三页,但是通过地址访问第四页的时候,&lt;code&gt;images&lt;/code&gt;是什么呢?&lt;/p&gt;

    &lt;p&gt;比如我这个相册,&lt;code&gt;https://www.douban.com/photos/album/102879847/?start=54&lt;/code&gt;,查看源码会发现&lt;code&gt;images&lt;/code&gt;是:&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;      '
	
	
  &amp;lt;/div&amp;gt;
	
  '     那么去掉这个字符串中的空格,只需判断下`images`是否等于`&amp;lt;/div&amp;gt;`.假设相册url后面那个`?start=x`中的x默认是0,那么从0开始,只要`images`的值不等于`&amp;lt;/div&amp;gt;`就把这个的url加到数组中.

  # -*- coding: utf-8 -*-
  import re
  import os
  import requests
  import urllib2
	
  album_url = 'https://www.douban.com/photos/album/102879847/'
  html = urllib2.urlopen(album_url).read()
  images = re.findall('&quot;photolst clearfix&quot;&amp;gt;(.*?)&amp;lt;div id=&quot;link-report',html,re.S)[0]
	
  index = 0
  album_list = []	#初始化数组
  while images != &quot;&amp;lt;/div&amp;gt;&quot;:   #no photos
      # 数据处理,把有照片的相册地址添加到数组中
      temp_url = album_url + '?start=' + str(index * 18)
      temp_html = urllib2.urlopen(temp_url).read()
      temp_images = re.findall('&quot;photolst clearfix&quot;&amp;gt;(.*?)&amp;lt;div id=&quot;link-report',temp_html,re.S)[0]
      #去空格
      images = temp_images.strip()	
      album_list.append(temp_url)
      index += 1
	    
  # 移除最后一个多余元素    
  album_list.pop()
  print album_list	
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;照片的相册地址都已经添加到数组里了,剩下的工作就是遍历这个数组,来下载所有的照片了.&lt;/p&gt;

    &lt;pre&gt;&lt;code&gt;  if not os.path.exists('pic'):
  j = 0
  # 遍历相册地址url
  for url in album_list:
      html = urllib2.urlopen(url).read()
      # match pics
      large = re.findall('&quot;photolst clearfix&quot;&amp;gt;(.*?)&amp;lt;div id=&quot;link-report', html, re.S)[0]
      pic_url = re.findall('img src=&quot;(.*?)&quot; /', large)
      j+=1
      i = 0
      # create folder
      if not os.path.exists('pic'):
          print '##开始下载.共' + str(imagesList.__len__() - 1) + '页##'
          os.makedirs('pic')
	
      for url in pic_url:
      # 之前下载的都是缩略图,如果想下载大图,那么把图片url中的`thumb`替换为`large`就行
          largeImg_url = url.replace('thumb', 'large')
          print '下载中...' + str(j) + '-' + str(i)
          pic = requests.get(largeImg_url)
          fp = open(os.getcwd() + '/pic/' + str(j) + '-' + str(i) + '.jpg', 'wb')
          fp.write(pic.content)
          fp.close()
          i += 1
          if i == pic_url.__len__():
              print '##第' + str(j) + '页下载完毕!!##'
  else:
      print '已下载过了!'	
&lt;/code&gt;&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;至此,分页下载的功能也已经实现了.如果人性化一点,可以让用户输入,相册地址来实现下载,只需要把相册的url作为input的入参就行了.&lt;br /&gt;
&lt;code&gt;album_url = raw_input('请输入豆瓣相册地址:')&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;###改换三方库
这里我使用了&lt;a href=&quot;https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html&quot;&gt;Beautiful Soup&lt;/a&gt;这个三方库,用法文档上写的很清楚了,这个库就是来替代正则截取HTML源码的工作,我们就不需要去找规律来截取想要的字段了,而是想要哪些内容,通过他的类名就可以获取诸如此类的人性化方法.&lt;/p&gt;

&lt;p&gt;更换后的代码全貌:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-
# __author__ = 'Heisenbean'
import os
import re
import requests
import urllib2
from bs4 import BeautifulSoup
from requests.packages.urllib3.exceptions import InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)


index = 0
base_url = raw_input('请输入豆瓣相册地址:')
url = base_url + '?start=0'
html = urllib2.urlopen(url).read()

#match pics
imagesList = []
soup = BeautifulSoup(html,'html.parser',from_encoding='utf-8')
node = soup.find_all('div', {'class': 'photo_wrap'})

while node:   #no photos
    imagesList.append(url)
    index += 1
    url = base_url + '?start=' + str(index * 18)
    html = urllib2.urlopen(url).read()
    soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')
    node = soup.find_all('div', {'class': 'photo_wrap'})

if not os.path.exists('pic'):
    j = 0
    for url in imagesList:
        html = urllib2.urlopen(url).read()
        # init picsList
        pic_url = []
        soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')
        node = soup.find_all('div', {'class': 'photo_wrap'})
        for imageUrl in node:
            for l in imageUrl.find_all('img'):
                pic_url.append(l.get('src'))
        j+=1
        i = 0
        # create folder
        if not os.path.exists('pic'):
            print '##开始下载.共' + str(imagesList.__len__() - 1) + '页##'
            os.makedirs('pic')

        for url in pic_url:
            largeImg_url = url.replace('thumb', 'large')
            print '下载中...' + str(j) + '-' + str(i)
            pic = requests.get(largeImg_url)
            fp = open(os.getcwd() + '/pic/' + str(j) + '-' + str(i) + '.jpg', 'wb')
            fp.write(pic.content)
            fp.close()
            i += 1
            if i == pic_url.__len__():
                print '##第' + str(j) + '页下载完毕!!##'
else:
    print '已下载过了!'
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里我就过多来介绍这个库的使用了,整篇文章也没有引入特别深的Python技能,都是一些基础的知识,目的就是能够希望有更多的人可以使用Python,喜欢上Python.&lt;/p&gt;

&lt;p&gt;毕竟,&lt;strong&gt;人生苦短,_______&lt;/strong&gt;.&lt;br /&gt;
最后,代码也放到&lt;a href=&quot;https://github.com/Heisenbean/Spiders&quot;&gt;Github&lt;/a&gt;了,如果觉得文章对你有帮助,请给一个&lt;strong&gt;star&lt;/strong&gt;.&lt;/p&gt;
</description>
        <pubDate>Mon, 16 Jun 2014 11:29:08 +0800</pubDate>
        <link>http://heisenbean.me/2014/06/doubanSpider/</link>
        <guid isPermaLink="true">http://heisenbean.me/2014/06/doubanSpider/</guid>
        
        
        <category>Python</category>
        
      </item>
    
      <item>
        <title>博客公告</title>
        <description>&lt;p&gt;我没有写技术博客的习惯,以前也就是在新浪博客写一些生活琐事和个人感悟什么的.但是个人是比较喜欢写作的.也较喜欢给人分享东西,大学的时候和朋友搞了一个电影资源网站,那个时候做的风生水起,最高成就网站全世界排行八万左右,因为网站没有任何广告,纯提供高清的片源和字幕受大家喜欢.后来因为各方面的打压,再加上朋友们都忙于生计对网站疏于管理,现在就剩我一个人上去分享一些自己喜欢的电影给大家.这种坚持,一是乐于分享的精神,二是对电影的热爱.&lt;/p&gt;

&lt;p&gt;后来接触了一些技术大牛,看到他们把自己的技术分享出来,而且写的特别好.然后就想自己是不是也可以把自己掌握的知识分享出来呢.&lt;/p&gt;

&lt;p&gt;前段时间在陈皓的博客上看到一篇名为&lt;a href=&quot;http://coolshell.cn/articles/11928.html&quot;&gt;&amp;lt;互联网之子 – Aaron Swartz&amp;gt;&lt;/a&gt;,讲述了一个叫亚伦 斯沃茨的互联网天才,大家有兴趣的话可以去看下关于亚伦的一部纪录片.我记得影片最后说,一个14岁的小男孩看到了亚伦死之前推广分享的一篇学术论文,想出了提前查出胰腺癌的方法,一般来说查出得了胰腺癌的时候就是死的时候了.亚伦其实主张的是互联网的开放和自由,因为在美国很多文献都不对外开放或者是收费的,但是他分享了那篇学术论文,正好又让一个能充分利用这篇论文的人看到了,这就是互联网分享的好处.所以我也决定,继续将分享的路走下去,希望对一些人有所帮助.&lt;/p&gt;

&lt;p&gt;-
2016年11月更新:&lt;/p&gt;

&lt;p&gt;之前用的服务器是在&lt;a href=&quot;http://digitalocean.com/&quot;&gt;DigitalOcean&lt;/a&gt;,但是自从我更新了一篇教广大网友如何免费科学上网的教程后,服务器就被DDos攻击了.和&lt;code&gt;DigitalOcean&lt;/code&gt;的客服交涉比较麻烦,近期又看到喵神把博客框架从&lt;a href=&quot;http://docs.ghostchina.com/zh/&quot;&gt;Ghost&lt;/a&gt;换成了&lt;a href=&quot;http://jekyllcn.com/&quot;&gt;Jekyll&lt;/a&gt;,于是也想把自己的换成&lt;code&gt;Jekyll&lt;/code&gt;,并打算不再使用&lt;code&gt;DigitalOcean&lt;/code&gt;,用&lt;code&gt;Github Pages&lt;/code&gt;.&lt;/p&gt;
</description>
        <pubDate>Sun, 20 Oct 2013 11:29:08 +0800</pubDate>
        <link>http://heisenbean.me/2013/10/bo-ke-gong-gao/</link>
        <guid isPermaLink="true">http://heisenbean.me/2013/10/bo-ke-gong-gao/</guid>
        
        
        <category>其他</category>
        
      </item>
    
  </channel>
</rss>
